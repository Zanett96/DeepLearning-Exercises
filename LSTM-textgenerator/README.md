# LSTMtextgenerator

The focus of this project is to develop a working text generator using a *Long Short Term Memory Recurrent Neural Network*. It's suggested to who has already some experience of neural networks, but you can eventually find all the theory you need [in this fantastic article](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) from Colah's blog. The model of the network can be found in the *net.py* file. The *text_and_train.py* file process the raw text and train the network. As a bonus, it also plot the embedding of the words, thus helping you determine how well your model is learning the semantic.  Finally, the *test.py* file generate some fresh text based on a seed passed from the user trough the command line. Keep in mind this is a **student-like** approach to LSTM text generation and should be taken accordingly. Shutout to this detailed [article](https://machinetalk.org/2019/02/08/text-generation-with-pytorch/) which helped me getting started! More open-source books to use as a dataset can be found in the [Project Gutenberg](https://www.gutenberg.org/) official website. Feel free to give me feedbacks. A Jupyter notebook that you can find in the project folder explains more in detail the reasoning behind the code to help you approaching the project. 
